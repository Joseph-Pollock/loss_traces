{
 "cells": [
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "source": [
    "from result_processing import get_lira_scores, create_bins, \\\n",
    "    get_overall_tpr_at_fpr,  \\\n",
    "    get_attackr_scores\n",
    "import pandas as pd\n",
    "import einops\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "\n",
    "plt.style.use('plot_style.mplstyle')\n"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "target_col = \"lira_score\"\n",
    "\n",
    "base_col = \"avg_norm\"\n",
    "levels = [0.1]\n",
    "\n",
    "def get_metrics_set_for_k(plots, k, levels, avg_norm_plots, base_col=\"avg_norm\", target_col=\"lira_score\"):\n",
    "    # avg_norm_plots = [x[0] for x in plots]\n",
    "    plot_metrics = []\n",
    "    for plot in plots:\n",
    "        metrics_list = []\n",
    "        # for metric in [\"auc\", \"overall_tpr\", \"precision\", \"recall\"]:\n",
    "        for metric in [\"auc\", \"overall_tpr\", \"precision\", \"recall\",\"precision_on_union\"]:\n",
    "            # if target_col != base_col:\n",
    "            f = plot[1]\n",
    "            if f is not None:\n",
    "                base_col = \"avg_norm\"\n",
    "            else:\n",
    "                base_col = plot[0]\n",
    "            metrics_list.append(get_dual_average(dual_count, exp_id, target_col, base_col, levels, f, k, metric))\n",
    "        plot_metrics.append(metrics_list)\n",
    "    return plot_metrics\n"
   ],
   "id": "56067ccd87dfb8fc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return super().default(obj)"
   ],
   "id": "e83274c6c25cc7da",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "def accuracy_at_k(df, k, members, top_score, level, attack_score=\"lira_score\", random_guessing=False):\n",
    "    n = int(k * (len(members) / 100.0))\n",
    "    top_attack = members.sort_values(attack_score, ascending=False)\n",
    "    return len(set(top_attack.head(n)['og_idx']).intersection(set(top_score.head(n)['og_idx']))) / float(n)\n",
    "\n",
    "\n",
    "def precision_at_k(df, k, members, top_score, level, attack_score=\"lira_score\", random_guessing=False):\n",
    "    n = int(k * (len(members) / 100.0))\n",
    "    attack_scores = df[attack_score]\n",
    "    fprs, _tprs, thresholds = metrics.roc_curve(df['target_trained_on'], attack_scores, drop_intermediate=False)\n",
    "    \n",
    "    threshold = thresholds[np.where(fprs <= level)[0][-1]]\n",
    "\n",
    "    if random_guessing:\n",
    "        return sum(members[attack_score] >= threshold)/len(members), n, sum(members[attack_score] >= threshold)\n",
    "\n",
    "    top_k = top_score.head(n)\n",
    "    tp = sum(top_k[attack_score] >= threshold)\n",
    "\n",
    "    return float(tp) / len(top_k), n, sum(members[attack_score] >= threshold)\n",
    "\n",
    "\n",
    "def recall_at_k(df, k, members, top_score, level, attack_score=\"lira_score\", random_guessing=False):\n",
    "    n = int(k * (len(members) / 100.0))\n",
    "    attack_scores = df[attack_score]\n",
    "    # if attack_score == \"attackr_score\":\n",
    "    #     attack_scores = 1 - attack_scores\n",
    "    fprs, _tprs, thresholds = metrics.roc_curve(df['target_trained_on'], attack_scores, drop_intermediate=False)\n",
    "    threshold = thresholds[np.where(fprs <= level)[0][-1]]\n",
    "\n",
    "    top_k = top_score.head(n)\n",
    "\n",
    "    if random_guessing:\n",
    "        return float(k)/100\n",
    "\n",
    "\n",
    "    # if attack_score == \"attackr_score\":\n",
    "    #     tp = sum(1-top_k[attack_score] >= threshold)\n",
    "    #     neg = sum(1-members[attack_score] >= threshold)\n",
    "    # else:\n",
    "    tp = sum(top_k[attack_score] >= threshold)\n",
    "    neg = sum(members[attack_score] >= threshold)\n",
    "\n",
    "    return tp / neg if neg else 0\n",
    "\n",
    "def precision_on_union(df, k, members, top_score, level, attack_score=\"lira_score\", random_guessing=False):\n",
    "    n = int(k * (len(members) / 100.0))\n",
    "    attack_scores = df[attack_score]\n",
    "    # if attack_score == \"attackr_score\":\n",
    "    #     attack_scores = 1-attack_scores\n",
    "    benchmarks = ['lira_score', 'attackr_score', 'rmia_score', 'loss attack']\n",
    "    members = df[df['target_trained_on'] == True]\n",
    "    members_dict = set()\n",
    "    \n",
    "    for attack_score in benchmarks:\n",
    "        \n",
    "        attack_scores = df[attack_score]\n",
    "        # if attack_score == \"attackr_score\":\n",
    "        #     attack_scores = 1-attack_scores\n",
    "            \n",
    "        fprs, _tprs, thresholds = metrics.roc_curve(df['target_trained_on'], attack_scores, drop_intermediate=False)\n",
    "        threshold = thresholds[np.where(fprs <= level)[0][-1]]\n",
    "        print(attack_score, threshold)\n",
    "\n",
    "        # if attack_score == \"attackr_score\":\n",
    "        #     vulnerable =  members.loc[(1 - members[attack_score]) >= threshold]\n",
    "        # else:\n",
    "        vulnerable = members.loc[members[attack_score] >= threshold][attack_score]  \n",
    "        members_dict.update(vulnerable.index)\n",
    "\n",
    "    if random_guessing:\n",
    "        return sum(members[attack_score] >= threshold)/len(members), n, sum(members[attack_score] >= threshold)\n",
    "\n",
    "    top_k = top_score.head(n)\n",
    "    # if attack_score==\"attackr_score\":\n",
    "    #     tp = sum(1-top_k[attack_score] >= threshold)\n",
    "    # else:\n",
    "    tp = len(set(top_k.index) & members_dict)\n",
    "    print(float(tp) / len(top_k), n, sum(members[attack_score] >= threshold))\n",
    "    return float(tp) / len(top_k), n, sum(members[attack_score] >= threshold)\n",
    "\n",
    "    "
   ],
   "id": "17f6fae1dc9ef78",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from results.result_processing import get_trace_reduction\n",
    "\n",
    "averaging_over_duals = True\n",
    "cross_evals = True\n",
    "# level = 0.00001\n",
    "# exp_id = 'WRN28_10_CIFAR100'\n",
    "# exp_id = 'WRN28_2_CINIC10'\n",
    "# exp_id = \"WRN40_4_CIFAR100\"\n",
    "exp_id = \"wrn28-2_CIFAR10\"\n",
    "dual_count = 1\n",
    "\n",
    "ks = [0.1, 1, 2, 3, 5]\n",
    "levels = np.logspace(-5, 0, 100)\n",
    "# levels = np.linspace(0,1,15)\n",
    "k=1\n",
    "metric = \"precision\"\n",
    "\n",
    "\n",
    "plots = [\n",
    "    ('rmia_1', lambda exp_id, t_id: get_rmia_scores(exp_id, \"target_1\", return_full_df=False)),\n",
    "    (\n",
    "        'final loss',\n",
    "        lambda exp_id, t_id: get_trace_reduction(exp_id, t_id, first=-1, reduction=None)\n",
    "    ),\n",
    "    (\n",
    "        'loss iqr (75%-25%)',\n",
    "        lambda exp_id, t_id: get_trace_reduction(exp_id, t_id, first=0, last=-1, reduction=\"iqr\")\n",
    "    ),\n",
    "]\n",
    "avg_norm_plots = [x[0] for x in plots]"
   ],
   "id": "e758f4022b216782",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import sklearn\n",
    "\n",
    "def get_dual_score(df, target_col, base_col, levels, k=1, metric=\"precision\"):\n",
    "    random_guessing = False\n",
    "    if metric == \"overall_tpr\":\n",
    "        return [get_overall_tpr_at_fpr(df, level, target_col=base_col) for level in levels]\n",
    "    if metric == \"auc\":\n",
    "        return [sklearn.metrics.roc_auc_score(df['target_trained_on'], df[base_col]) for level in levels]\n",
    "\n",
    "    members = df[df['target_trained_on'] == True]\n",
    "    # if base_col == \"attackr_score\":\n",
    "    #     top_score = members.sort_values(base_col, ascending=True)\n",
    "    if base_col == \"random_guessing\":\n",
    "        top_score = members.sort_values(\"lira_score\", ascending=False)\n",
    "        random_guessing = True\n",
    "    else:\n",
    "        top_score = members.sort_values(base_col, ascending=False)\n",
    "\n",
    "    if metric == \"precision\":\n",
    "        metric_func = precision_at_k\n",
    "    elif metric == \"accuracy\":\n",
    "        metric_func = accuracy_at_k\n",
    "    elif metric == \"recall\":\n",
    "        metric_func = recall_at_k\n",
    "    elif metric == \"precision_on_union\":\n",
    "        metric_func = precision_on_union\n",
    "\n",
    "    metrics = [metric_func(df, k, members, top_score, level, target_col, random_guessing) for level in levels]\n",
    "    # print(metrics)\n",
    "    if metric == \"precision\" or metric == \"precision_on_union\":\n",
    "        return [x[0] for x in metrics]\n",
    "    if metric == \"n_points\":\n",
    "        return [x[:1] for x in metrics]\n",
    "    if metric == \"n_most_vulnerable\":\n",
    "        return [[x[1:]] for x in metrics]\n",
    "    return metrics"
   ],
   "id": "446d93f75b967ed4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from results.result_processing import get_rmia_scores\n",
    "\n",
    "\n",
    "def get_dual_average(dual_count, exp_id, target_col, base_col,levels, f=None, k=1, metric=\"precision\"):\n",
    "    duals = []\n",
    "\n",
    "    for d in range(dual_count):\n",
    "        tag = f'dual_track_both_{d}' if dual_count > 1 else ''\n",
    "        print(tag)\n",
    "        df = get_lira_scores(exp_id)\n",
    "                             # + \"_checkpoint_before_100\", f'dual_track_both_{d}')\n",
    "        df['attackr_score'] = get_attackr_scores(exp_id + tag)\n",
    "        df['rmia_score'] = get_rmia_scores(exp_id + tag)\n",
    "        df['loss attack'] = -get_trace_reduction(exp_id, first=-1)\n",
    "        df[\"lt_iqr\"] =  get_trace_reduction(exp_id, reduction=\"iqr\") \n",
    "        # df['avg_norm'] = f(exp_id, f'dual_track_both_{d}')    # FROM SRC MODEL\n",
    "        \n",
    "        \n",
    "        if f is not None:\n",
    "            # print(base_col)\n",
    "            df['avg_norm'] = f(exp_id, tag)\n",
    "            df['norm_bin'] = create_bins(df)\n",
    "            base_col = \"avg_norm\"\n",
    "\n",
    "        duals.append([round(x, 3) for x in get_dual_score(df,target_col,base_col,levels, k, metric)])\n",
    "    return einops.reduce(np.array(duals), \"duals fprs -> fprs\", reduction=\"mean\")"
   ],
   "id": "5d105b7077cccf57",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_tpr_fpr_attack_comparison(dual_count, exp_id, k, levels,metric=\"precision\"):\n",
    "    metric_dict = {}\n",
    "    for target_col in [\"lira_score\"]:\n",
    "    # for target_col in [\"attackr_score\"]:\n",
    "        plt.figure()\n",
    "        metric_dict[target_col] = {}\n",
    "\n",
    "        for idx, base_col in enumerate([x[0] for x in plots] + [\"lira_score\", \"attackr_score\", \"rmia_score\",\"random_guessing\"]):\n",
    "            if target_col != base_col:\n",
    "                print(f\"{target_col=}{base_col=}\")\n",
    "                f = None\n",
    "                if base_col in avg_norm_plots:\n",
    "                    f = plots[idx][1]\n",
    "                tpr = get_dual_average(dual_count, exp_id, target_col, base_col, levels, f, k, metric)\n",
    "                metric_dict[target_col][base_col] = tpr\n",
    "        return metric_dict\n",
    "    \n",
    "label_mapping = {\"lira_score\": \"LiRA\", \"final loss\":\"Final loss\", \"loss iqr (75%-25%)\": \"LT-IQR (75%-25%)\", \"attackr_score\":\"Attack R\", \"rmia_score\": \"RMIA\", \"random_guessing\":\"Random guessing\", \"rmia_1\": \"RMIA (1 shadow model)\"}\n",
    "\n",
    "def plot_tpr(metric, metric_dict, log=True):\n",
    "    k = 1\n",
    "    levels = np.logspace(-5, 0, 100) if log else np.linspace(0, 1, 100)\n",
    "    \n",
    "    for target_col, v in metric_dict.items():\n",
    "        for base_col, vals in v.items():\n",
    "            if target_col != base_col:\n",
    "                if base_col == \"random_guessing\":\n",
    "                    plt.plot(levels, vals, '--', label=label_mapping[base_col])\n",
    "                else:\n",
    "                    plt.plot(levels, vals, label=label_mapping[base_col])\n",
    "        plt.xlabel(\"FPR\")\n",
    "        plt.ylabel(f\"{metric.capitalize()}@{k}%\")\n",
    "        plt.legend()\n",
    "        # [p + f\"{label}\" for p in avg_norm_plots for k in ks]\n",
    "        # plt.axvline(x = 0.001, color = 'b', label = 'axvline - full height')\n",
    "        if log:\n",
    "            plt.xscale('log')\n",
    "            \n",
    "        # plt.ylim((0,0.5))\n",
    "        l = \"log\" if log else \"lin\"\n",
    "        plt.savefig(f\"./{metric}@{k}_{target_col}_fpr_{l}_{exp_id}.pdf\", bbox_inches='tight')\n",
    "        plt.show()    \n"
   ],
   "id": "aff1a908271b9fb0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# from results.result_processing import get_reduced_data\n",
    "import seaborn as sns\n",
    "level = 0.001\n",
    "\n",
    "label_mapping = {\"lira_score\": \"LiRA\", \"loss attack\":\"Loss Attack\", \"final loss\":\"Final loss\", \"loss iqr (75%-25%)\": \"LT-IQR (75%-25%)\", \"attackr_score\":\"Attack R\", \"rmia_score\": \"RMIA\", \"random_guessing\":\"Random guessing\"}\n",
    "\n",
    "def get_matrix_attack_comparison(exp_id, level):\n",
    "    df = get_lira_scores(exp_id)\n",
    "    df['attackr_score'] = get_attackr_scores(exp_id)\n",
    "    df['rmia_score'] = get_rmia_scores(exp_id)['rmia_2.0_score']\n",
    "    df['loss attack'] = -get_trace_reduction(exp_id, first=-1)\n",
    "    df[\"lt_iqr\"] =  get_trace_reduction(exp_id, reduction=\"iqr\")\n",
    "    \n",
    "    \n",
    "    benchmarks = ['lira_score', 'attackr_score', 'rmia_score', 'loss attack']\n",
    "    members = df[df['target_trained_on'] == True]\n",
    "    members_dict = {}\n",
    "    \n",
    "    for attack_score in benchmarks:\n",
    "        \n",
    "        attack_scores = df[attack_score]\n",
    "        # if attack_score == \"attackr_score\":\n",
    "        #     attack_scores = 1-attack_scores\n",
    "            \n",
    "        fprs, _tprs, thresholds = metrics.roc_curve(df['target_trained_on'], attack_scores, drop_intermediate=False)\n",
    "        threshold = thresholds[np.where(fprs <= level)[0][-1]]\n",
    "        print(attack_score, threshold)\n",
    "\n",
    "        # if attack_score == \"attackr_score\":\n",
    "        #     vulnerable =  members.loc[(1 - members[attack_score]) >= threshold]\n",
    "        # else:\n",
    "        vulnerable = members.loc[members[attack_score] >= threshold][attack_score]\n",
    "            \n",
    "        members_dict[attack_score] = vulnerable\n",
    "        \n",
    "    \n",
    "    overlap_matrix = [[len(list(set(b.index) & set(a.index))) for k, a in members_dict.items()] for y, b in members_dict.items()]\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    # sns.heatmap(overlap_matrix, \n",
    "    #             annot=True, \n",
    "    #             fmt='g',\n",
    "    #             cmap = sns.color_palette(\"ch:start=.2,rot=-.3,dark=0.3,light=0.95_r\", as_cmap=True),\n",
    "    #             xticklabels=[label_mapping[b] for b in benchmarks],\n",
    "    #             yticklabels=[label_mapping[b] for b in benchmarks],\n",
    "    #             square=True)\n",
    "    # \n",
    "    # plt.title(f'Overlap Matrix for {exp_id} at FPR={level}')\n",
    "    # plt.savefig(f\"./overlap_matrix_fpr={level}.pdf\", \n",
    "    #                    bbox_inches='tight',\n",
    "    #                    dpi=300)\n",
    "    return members_dict\n",
    "# \n",
    "for level in [0.001]:\n",
    "    members_dict = get_matrix_attack_comparison(exp_id, level)\n",
    "    "
   ],
   "id": "f53c90115d1434b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "len(set(members_dict[\"lira_score\"].index) & set(members_dict[\"rmia_score\"].index) & set(members_dict[\"attackr_score\"].index))",
   "id": "687e4d39ffd007a7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# from results.result_processing import get_reduced_data\n",
    "import seaborn as sns\n",
    "level = 0.001\n",
    "\n",
    "label_mapping = {\"lira_score\": \"LiRA\", \"loss attack\":\"Loss Attack\", \"final loss\":\"Final loss\", \"loss iqr (75%-25%)\": \"LT-IQR (75%-25%)\", \"attackr_score\":\"Attack R\", \"rmia_score\": \"RMIA\", \"random_guessing\":\"Random guessing\"}\n",
    "\n",
    "def get_matrix_attack_comparison(exp_id, level):\n",
    "    df = get_lira_scores(exp_id)\n",
    "    df['attackr_score'] = get_attackr_scores(exp_id)\n",
    "    df['rmia_score'] = get_rmia_scores(exp_id)['rmia_2.0_score']\n",
    "    df['loss attack'] = -get_trace_reduction(exp_id, first=-1)\n",
    "    df[\"lt_iqr\"] =  get_trace_reduction(exp_id, reduction=\"iqr\") \n",
    "    \n",
    "    \n",
    "    benchmarks = ['lira_score', 'attackr_score', 'rmia_score', 'loss attack']\n",
    "    members = df[df['target_trained_on'] == True]\n",
    "    members_dict = {}\n",
    "    \n",
    "    for attack_score in benchmarks:\n",
    "        \n",
    "        attack_scores = df[attack_score]\n",
    "\n",
    "        fprs, _tprs, thresholds = metrics.roc_curve(df['target_trained_on'], attack_scores, drop_intermediate=False)\n",
    "        threshold = thresholds[np.where(fprs <= level)[0][-1]]\n",
    "        print(attack_score, threshold)\n",
    "\n",
    "        vulnerable = members.loc[members[attack_score] >= threshold][attack_score]\n",
    "            \n",
    "        members_dict[attack_score] = vulnerable\n",
    "        \n",
    "    \n",
    "    overlap_matrix = [[len(list(set(b.index) & set(a.index))) for k, a in members_dict.items()] for y, b in members_dict.items()]\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(overlap_matrix, \n",
    "                annot=True, \n",
    "                fmt='g',\n",
    "                cmap = sns.color_palette(\"ch:start=.2,rot=-.3,dark=0.3,light=0.95_r\", as_cmap=True),\n",
    "                xticklabels=[label_mapping[b] for b in benchmarks],\n",
    "                yticklabels=[label_mapping[b] for b in benchmarks],\n",
    "                square=True)\n",
    "\n",
    "    plt.title(f'Overlap Matrix for {exp_id} at FPR={level}')\n",
    "    plt.savefig(f\"./overlap_matrix_fpr={level}.pdf\", \n",
    "                       bbox_inches='tight',\n",
    "                       dpi=300)\n",
    "    return members_dict\n",
    "# \n",
    "for level in [0.001]:\n",
    "    members_dict = get_matrix_attack_comparison(exp_id, level)"
   ],
   "id": "92097b2df0be1055",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def print_overall_tpr_at_fpr(df: pd.DataFrame, target_col=\"lira_score\"):\n",
    "    # if target_col == \"attackr_score\":\n",
    "    #     df[target_col] =  1-df[target_col]\n",
    "    fpr, tpr, _thresholds = metrics.roc_curve(df['target_trained_on'], df[target_col], drop_intermediate=False)\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.xlim(left=0)\n",
    "    # plt.xscale('log')\n",
    "\n",
    "exp_id = \"wrn28-2_CIFAR10\"\n",
    "    \n",
    "df = get_lira_scores(exp_id)\n",
    "df[\"rmia_score\"] = get_rmia_scores(exp_id)\n",
    "df[\"attackr_score\"] = get_attackr_scores(exp_id)\n",
    "df[\"lt_iqr\"] =  get_trace_reduction(exp_id, reduction=\"iqr\") \n",
    "df[\"loss_attack\"]  = -get_trace_reduction(exp_id, first=-1, reduction=None)\n",
    "# print_overall_tpr_at_fpr(df, \"lt_iqr\")\n",
    "\n",
    "plt.figure()\n",
    "for a in [\"rmia_score\", \"attackr_score\", \"lira_score\"]:\n",
    "    fpr, tpr, _thresholds = metrics.roc_curve(df['target_trained_on'], df[a], drop_intermediate=False)\n",
    "    plt.plot(fpr, tpr)\n",
    "    # plt.xlim(left=0)\n",
    "plt.axline((0, 0), slope=1)\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.ylim(bottom=0.001)\n",
    "plt.xlim(left=0.001)\n",
    "\n",
    "plt.legend([\"rmia_score\", \"attackr_score\", \"lira_score\"])"
   ],
   "id": "71c3b058af76a705",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "k=1\n",
    "levels = np.logspace(-5, 0, 100)\n",
    "precision_dict = get_tpr_fpr_attack_comparison(1, exp_id, k, levels, \"precision\")\n",
    "recall_dict = get_tpr_fpr_attack_comparison(1, exp_id, k, levels, \"recall\")\n",
    "\n",
    "file = f\"precision_dict_log_{exp_id}.json\"\n",
    "with open(file, 'w') as f: \n",
    "    json.dump(precision_dict, f, cls=NumpyEncoder)\n",
    "\n",
    "file = f\"recall_dict_log_{exp_id}.json\"\n",
    "with open(file, 'w') as f: \n",
    "    json.dump(recall_dict, f, cls=NumpyEncoder)\n",
    "\n",
    "k=1\n",
    "levels = np.linspace(0, 1, 100)\n",
    "print(exp_id)\n",
    "precision_dict = get_tpr_fpr_attack_comparison(1, exp_id, k, levels, \"precision\")\n",
    "recall_dict = get_tpr_fpr_attack_comparison(1, exp_id, k, levels, \"recall\")\n",
    "\n",
    "file = f\"precision_dict_lin_{exp_id}.json\"\n",
    "with open(file, 'w') as f: \n",
    "    json.dump(precision_dict, f, cls=NumpyEncoder)\n",
    "\n",
    "file = f\"recall_dict_lin_{exp_id}.json\"\n",
    "with open(file, 'w') as f: \n",
    "    json.dump(recall_dict, f, cls=NumpyEncoder)\n",
    "\n",
    "# with open(file, 'r') as f:\n",
    "#     precision_dict = json.load(f)"
   ],
   "id": "dce11f84125b8221",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import json\n",
    "with open(f\"precision_dict_log_{exp_id}.json\", 'r') as f:\n",
    "    precision_dict = json.load(f)\n",
    "plot_tpr(metric=\"precision\", metric_dict=precision_dict, log=True)\n",
    "with open(f\"precision_dict_lin_{exp_id}.json\", 'r') as f:\n",
    "    precision_dict = json.load(f)\n",
    "plot_tpr(metric=\"precision\", metric_dict=precision_dict, log=False)"
   ],
   "id": "9c9bba78f53905df",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with open(f\"recall_dict_log_{exp_id}.json\", 'r') as f:\n",
    "    recall_dict = json.load(f)\n",
    "plot_tpr(metric=\"recall\", metric_dict=recall_dict, log=True)\n",
    "with open(f\"recall_dict_lin_{exp_id}.json\", 'r') as f:\n",
    "    recall_dict = json.load(f)\n",
    "plot_tpr(metric=\"recall\", metric_dict=recall_dict, log=False)"
   ],
   "id": "4cf5fce62a12fd7b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "plots = [\n",
    "    (\n",
    "    'loss attack',\n",
    "    lambda exp_id, t_id: -get_trace_reduction(exp_id, t_id, first=-1, reduction=None)\n",
    "    ),\n",
    "    (\n",
    "    'final loss',\n",
    "    lambda exp_id, t_id: get_trace_reduction(exp_id, t_id, first=-1, reduction=None)\n",
    "    ),\n",
    "    (\n",
    "    'delta/mid',\n",
    "    lambda exp_id, t_id: get_trace_reduction(exp_id, t_id, first=0, last=-1, reduction=\"delta/mid\")\n",
    "    ),\n",
    "    (\n",
    "    'mean loss',\n",
    "    lambda exp_id, t_id: get_trace_reduction(exp_id, t_id, last=-1, reduction=\"mean\")\n",
    "    ),\n",
    "    (\n",
    "    'final - init (loss)',\n",
    "    lambda exp_id, t_id: get_trace_reduction(exp_id, t_id, first=-1, reduction=None).iloc[:, 0] - get_trace_reduction(exp_id, t_id, first=0, last=1, reduction=None).iloc[:, 0]\n",
    "    ),\n",
    "    (\n",
    "    'final loss / init loss',\n",
    "    lambda exp_id, t_id: get_trace_reduction(exp_id, t_id, first=-1) / get_trace_reduction(exp_id, t_id, last=1)\n",
    "    ),\n",
    "    (\n",
    "    'avg quarter 2 - avg late (loss)',\n",
    "    lambda exp_id, t_id: get_trace_reduction(exp_id, t_id, first=5, last=10, reduction=\"mean\") - get_trace_reduction(exp_id, t_id, first=-2, reduction=\"mean\")\n",
    "    ),\n",
    "    (\n",
    "    'loss iqr (75%-25%)',\n",
    "    lambda exp_id, t_id: get_trace_reduction(exp_id, t_id, first=0, last=-1, reduction=\"iqr\")\n",
    "    ),\n",
    "    (\n",
    "    'mid-end',\n",
    "    lambda exp_id, t_id: get_trace_reduction(exp_id, t_id, last=-1, reduction=\"mid-end\")\n",
    "    ),\n",
    "    # (\n",
    "    # '|avg 1st half loss - avg late/final loss|',\n",
    "    # lambda exp_id, t_id: np.abs(get_trace_reduction(exp_id, t_id, first=1, last=10, reduction=\"mean\") - get_trace_reduction(exp_id, t_id, first=-2, reduction=\"mean\"))\n",
    "    #  ),\n",
    "    # (\n",
    "    # 'final loss / avg loss',\n",
    "    # lambda exp_id, t_id: get_trace_reduction(exp_id, t_id, first=-1, reduction=None) / get_trace_reduction(exp_id, t_id, first=0, reduction=\"mean\")\n",
    "    #  ),\n",
    "    # (\n",
    "    # 'loss norm',\n",
    "    # lambda exp_id, t_id: get_trace_reduction(exp_id, t_id, reduction=\"norm1\")\n",
    "    #  ),\n",
    "]\n",
    "\n",
    "def make_metrics_table(k_metrics, ks, avg_norm_plots):\n",
    "    # avg_norm_plots =  [x[0] for x in plots]\n",
    "    # k_metrics = [get_metrics_set_for_k(plots, k, levels, avg_norm_plots) for k in ks]\n",
    "    \n",
    "    for idx,i in enumerate(ks):\n",
    "        print(ks)\n",
    "        plot_df = k_metrics[idx]\n",
    "        plot_df = pd.DataFrame(np.array(plot_df)[:,:,0])\n",
    "        plot_df.columns = [f\"AUC\", f\"TPR@FPR={levels[0]}\", f\"Precision@{i}% (FPR={levels[0]})\", f\"Recall@{i}% (FPR={levels[0]})\",f\"Precision_on_union@{i}% (FPR={levels[0]})\"]\n",
    "        # plot_df.columns = [f\"Precision_on_union@{i}% (FPR={levels[0]})\", f\"Precision@{i}% (FPR={levels[0]})\"]\n",
    "        # display(plot_df)\n",
    "        \n",
    "        plot_df.index = avg_norm_plots\n",
    "        base_col = plots[0][0]\n",
    "        target_col = \"lira_score\"\n",
    "        f = None\n",
    "\n",
    "        display(plot_df)\n",
    "    # return k_metrics\n",
    "\n",
    "# make_metrics_table([0.1, 1, 2, 3, 5], [0.001])"
   ],
   "id": "63db572d099a0e73",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ks =  [1, 3, 5]\n",
    "levels = [0.001]\n",
    "avg_norm_plots =  [x[0] for x in plots]\n",
    "\n",
    "k_metrics = [get_metrics_set_for_k(plots , k, levels, avg_norm_plots) for k in ks]\n",
    "np.save(f\"k_metrics_table_{exp_id}_test.npy\", np.array(k_metrics))\n",
    "k_metrics = np.load(f'k_metrics_table_{exp_id}_test.npy')\n",
    "make_metrics_table(k_metrics, ks, avg_norm_plots)"
   ],
   "id": "716a214d3b0fa2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ks =  [1, 3, 5, 10]\n",
    "levels = [0.001]\n",
    "\n",
    "def get_attack_nshadows(exp_id, tag):\n",
    "    return get_lira_scores(exp_id, tag)[\"lira_score\"]\n",
    "\n",
    "def get_lira_32(exp_id, _):\n",
    "    return get_attack_nshadows(exp_id, \"lira_offline_32\")\n",
    "\n",
    "plots = [\n",
    "            # ('attackr_score', None), \n",
    "            ('lira_128_online', lambda exp_id, t_id: get_lira_scores(exp_id, \"target\", return_full_df=False)),\n",
    "            ('lira_32_online', lambda exp_id, t_id: get_lira_scores(exp_id, \"target_32\", return_full_df=False)),\n",
    "            ('lira_8_online', lambda exp_id, t_id: get_lira_scores(exp_id, \"target_8\", return_full_df=False)),\n",
    "            ('lira_128_offline', lambda exp_id, t_id: get_lira_scores(exp_id, \"target_offline_128\", return_full_df=False)),\n",
    "            ('lira_32_offline', lambda exp_id, t_id: get_lira_scores(exp_id, \"target_offline_32\", return_full_df=False)),\n",
    "            ('lira_8_offline', lambda exp_id, t_id: get_lira_scores(exp_id, \"target_offline_8\", return_full_df=False)),\n",
    "            ('attackr_128', lambda exp_id, t_id: get_attackr_scores(exp_id, \"target\", return_full_df=False)),\n",
    "            ('attackr_32', lambda exp_id, t_id: get_attackr_scores(exp_id, \"target_32\", return_full_df=False)),\n",
    "            ('attackr_8', lambda exp_id, t_id: get_attackr_scores(exp_id, \"target_8\", return_full_df=False)),\n",
    "            ('rmia_128', lambda exp_id, t_id: get_rmia_scores(exp_id, \"target\", return_full_df=False)),\n",
    "            ('rmia_32', lambda exp_id, t_id: get_rmia_scores(exp_id, \"target_32\", return_full_df=False)),\n",
    "            ('rmia_8', lambda exp_id, t_id: get_rmia_scores(exp_id, \"target_8\", return_full_df=False)),\n",
    "            ('rmia_1', lambda exp_id, t_id: get_rmia_scores(exp_id, \"target_1\", return_full_df=False)),\n",
    "            (\n",
    "            'loss attack',\n",
    "            lambda exp_id, t_id: -get_trace_reduction(exp_id, t_id, first=-1, reduction=None)\n",
    "            ),\n",
    "            ('loss iqr (75%-25%)',\n",
    "            lambda exp_id, t_id: get_trace_reduction(exp_id, t_id, first=0, last=-1, reduction=\"iqr\")\n",
    "            ),\n",
    "            \n",
    "]\n",
    "\n",
    "avg_norm_plots =  [x[0] for x in plots]\n",
    "\n",
    "k_metrics = [get_metrics_set_for_k(plots , k, levels, avg_norm_plots) for k in ks]\n",
    "np.save(f\"k_metrics_table_attacks_{exp_id}_t.npy\", np.array(k_metrics))\n",
    "k_metrics = np.load(f'k_metrics_table_attacks_{exp_id}_t.npy')\n",
    "make_metrics_table(k_metrics, ks, avg_norm_plots)"
   ],
   "id": "dc7f6f1456a2ccdc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ks = [0.1, 1, 2, 3, 5]\n",
    "levels = np.logspace(-5, 0, 100)\n",
    "recall_for_k_dict = plot_tpr_fpr_us_comparison(dual_count, exp_id, ks, levels,metric=\"recall\", log=True)\n",
    "\n",
    "file = f\"recall_for_k_dict_{exp_id}_log.json\"\n",
    "with open(file, 'w') as f: \n",
    "    json.dump(recall_for_k_dict, f, cls=NumpyEncoder)\n",
    "\n",
    "ks = [0.1, 1, 2, 3, 5]\n",
    "levels = np.linspace(0, 1, 100)\n",
    "recall_for_k_dict = plot_tpr_fpr_us_comparison(dual_count, exp_id, ks, levels,metric=\"recall\", log=False)\n",
    "\n",
    "file = f\"recall_for_k_dict_{exp_id}_lin.json\"\n",
    "with open(file, 'w') as f: \n",
    "    json.dump(recall_for_k_dict, f, cls=NumpyEncoder)"
   ],
   "id": "4edaabb185082d14",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with open(f\"precision_for_k_dict_{exp_id}_log.json\", 'r') as f:\n",
    "    precision_for_k_dict = json.load(f)\n",
    "plot_metrics_for_k(ks,levels,precision_for_k_dict, log=True)\n",
    "\n",
    "with open(f\"precision_for_k_dict_{exp_id}_lin.json\", 'r') as f:\n",
    "    precision_for_k_dict = json.load(f)\n",
    "plot_metrics_for_k(ks,levels,precision_for_k_dict, log=False)"
   ],
   "id": "6595027a4df960b8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with open(f\"recall_for_k_dict_{exp_id}_log.json\", 'r') as f:\n",
    "    recall_for_k_dict = json.load(f)\n",
    "plot_metrics_for_k(ks,levels,recall_for_k_dict, log=True, metric=\"recall\")\n",
    "\n",
    "with open(f\"recall_for_k_dict_{exp_id}_lin.json\", 'r') as f:\n",
    "    recall_for_k_dict = json.load(f)\n",
    "plot_metrics_for_k(ks,levels,recall_for_k_dict, log=False, metric=\"recall\")"
   ],
   "id": "aa72b44cb28aca0f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# def make_metrics_plot(plots, ks, levels):\n",
    "ks= [0.1, 1, 2, 3, 5]\n",
    "levels=[0.001]\n",
    "avg_norm_plots = [x[0] for x in plots]\n",
    "k_metrics = [get_metrics_set_for_k(plots, k, levels) for k in ks]\n",
    "\n",
    "for i,k_val in enumerate(ks):\n",
    "    plot_df = k_metrics[i]\n",
    "    plot_df = pd.DataFrame(np.array(plot_df)[:,:,0])\n",
    "    plot_df.columns = [f\"Accuracy@{i}%\", f\"Precision@{i}% (FPR={levels[0]})\", f\"Recall@{i}% (FPR={levels[0]})\"]\n",
    "    plt.plot(plot_df['Accuracy@{i}%'])"
   ],
   "id": "b60f2bd57b2b545f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_tpr_fpr_us_comparison(dual_count, exp_id, ks, levels, metric=\"precision\", log=True):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.colors as mcolors\n",
    "    import numpy as np\n",
    "    \n",
    "    avg_norm_plots = [x[0] for x in plots]\n",
    "    \n",
    "    for target_col in [\"lira_score\"]:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        # Create colormap\n",
    "        colors = plt.cm.viridis(np.linspace(0, 1, len(ks)))\n",
    "        \n",
    "        for idx, base_col in enumerate([x[0] for x in plots]):\n",
    "            tpr_values = []  # Store all TPR values to use for filling\n",
    "            \n",
    "            # First collect all TPR values\n",
    "            for k_idx, k in enumerate(ks):\n",
    "                if target_col != base_col:\n",
    "                    f = None\n",
    "                    if base_col in avg_norm_plots:\n",
    "                        f = plots[idx][1]\n",
    "                    tpr = get_dual_average(dual_count, exp_id, target_col, base_col, levels, f, k, metric)\n",
    "                    tpr_values.append(tpr)\n",
    "            # print(levels, tpr_values)\n",
    "            # Fill the area between zero and the lowest curve\n",
    "            plt.fill_between(levels, np.zeros_like(levels), tpr_values[0], \n",
    "                           color=colors[0],\n",
    "                           alpha=0.3)\n",
    "            \n",
    "            # Plot fills between consecutive curves\n",
    "            for i in range(len(ks) - 1):\n",
    "                plt.fill_between(levels, tpr_values[i], tpr_values[i+1], \n",
    "                               color=colors[i],\n",
    "                               alpha=0.3)\n",
    "                \n",
    "            # Plot the lines on top for better visibility\n",
    "            for k_idx, k in enumerate(ks):\n",
    "                if base_col in avg_norm_plots:\n",
    "                    label = base_col + f\"_{k}\"\n",
    "                plt.plot(levels, tpr_values[k_idx], \n",
    "                        label = f\"k={k}%\",\n",
    "                        color=colors[k_idx],\n",
    "                        linewidth=2)\n",
    "        \n",
    "        plt.xlabel(\"FPR\")\n",
    "        plt.ylabel(f\"Precision@k%\")\n",
    "        plt.legend()\n",
    "        \n",
    "        if log:\n",
    "            plt.xscale('log')\n",
    "            l = \"log\"\n",
    "        else:\n",
    "            l = \"lin\"\n",
    "            \n",
    "        plt.grid(True, which=\"both\", ls=\"-\", alpha=0.2)\n",
    "        plt.savefig(f\"./{metric}@different_ks_filled_{target_col}_fpr_{l}.pdf\", \n",
    "                   bbox_inches='tight',\n",
    "                   dpi=300)\n",
    "        plt.show()"
   ],
   "id": "284f6dee3471dc0c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ks = [0.1, 1, 2, 3, 5]\n",
    "levels = np.logspace(-5, 0, 100)\n",
    "plot_tpr_fpr_us_comparison(dual_count, exp_id, ks, levels,metric=\"precision\", log=True)"
   ],
   "id": "4a2fb17eee0a524c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ks = [0.1, 1, 2, 3, 5]\n",
    "levels = np.linspace(0, 1, 100)\n",
    "plot_tpr_fpr_us_comparison(dual_count, exp_id, ks, levels,metric=\"precision\", log=False)"
   ],
   "id": "ed4c1e90ac437555",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from config import STORAGE_DIR\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from result_processing import get_lira_scores\n",
    "import os\n",
    "from sklearn import metrics\n",
    "\n",
    "pcts = np.linspace(0, 1, 21)\n",
    "\n",
    "a_pcts = [p for p in pcts if p >= 0.5]\n",
    "b_pcts = [p for p in pcts if p <= 0.5]\n",
    "iqr_pairs = [(a, b) for a in a_pcts for b in b_pcts]\n",
    "\n",
    "plots = iqr_pairs \n",
    "\n",
    "iqr_precisions = {p: [] for p in iqr_pairs}\n",
    "   \n",
    "exp_id = 'wrn28-2_CIFAR10'\n",
    "model_ids = ['WRN28-2']\n",
    "dual_counts = [1]\n",
    "   \n",
    "def get_trace_reduction(exp_id: str, target_id: str = None, l: int = 0.25, r: int = 0.75, trace_type=\"losses\"):\n",
    "\n",
    "    base_name = exp_id + '_' + target_id if target_id else exp_id\n",
    "    path = os.path.join(STORAGE_DIR, trace_type, base_name + '.pq')\n",
    "    df = pd.read_parquet(path)\n",
    "\n",
    "    return _reduction_iqr(df, l, r)\n",
    "   \n",
    "def _reduction_iqr(traces: pd.DataFrame, l, r) -> pd.Series:\n",
    "    \"\"\"Calculate interquartile range.\"\"\"\n",
    "    q1 = traces.quantile(r, axis=1)\n",
    "    q3 = traces.quantile(l, axis=1)\n",
    "    return q3 - q1g\n",
    "\n",
    "fig, axes = plt.subplots(len(exp_ids), len(plots), figsize=(1 + (5 * len(plots)),(5 * len(exp_ids))), squeeze=False)\n",
    "num_bins=100\n",
    "# ks = range(1,101)\n",
    "# ks = np.arange(0.1, 5+.1, 0.1)\n",
    "ks = [1]\n",
    "for j, (left, right) in enumerate(plots):\n",
    "\n",
    "      df = get_lira_scores(exp_id)\n",
    "      df['avg_norm'] = get_trace_reduction(exp_id, l=left, r=right)\n",
    "\n",
    "      members = df[df['target_trained_on'] == True]\n",
    "      top_lira = members.sort_values('lira_score', ascending=False)\n",
    "      top_score = members.sort_values('avg_norm', ascending=False)\n",
    "      \n",
    "\n",
    "      def precision_at_k(k):\n",
    "         n = int(k * (len(members) / 100.0))\n",
    "         fprs, tprs, _thresholds = metrics.roc_curve(df['target_trained_on'], df['lira_score'], drop_intermediate=False)\n",
    "         threshold = _thresholds[np.where(fprs<=level)[0][-1]]\n",
    "         # print(iqr_pairs[j], threshold)\n",
    "         top_k = top_score.head(n)\n",
    "         tp = sum(top_k['lira_score'] >= threshold)\n",
    "         return float(tp) / len(top_k)\n",
    "    \n",
    "      print(left, right, iqr_pairs[j],precision_at_k(1))\n",
    "      iqr_precisions[iqr_pairs[j]].append(precision_at_k(1))\n",
    "      \n",
    "data = np.reshape(np.array(list(iqr_precisions.values())).mean(axis=1), (len(b_pcts), len(a_pcts)))\n"
   ],
   "id": "952da5b145bf46e7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import seaborn\n",
    "\n",
    "ax = seaborn.heatmap(data, annot=True, xticklabels=[f'{x:.2f}'for i, x in enumerate(b_pcts)], yticklabels=[f'{x:.2f}'for x in a_pcts])\n",
    "ax.invert_yaxis()\n",
    "for label in ax.xaxis.get_ticklabels()[1::2]:\n",
    "    label.set_visible(False)\n",
    "for label in ax.yaxis.get_ticklabels()[1::2]:\n",
    "    label.set_visible(False)\n",
    "\n",
    "plt.xlabel(r'$q_1$')\n",
    "plt.ylabel(r'$q_2$')"
   ],
   "id": "9cf546448e20fae6",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
